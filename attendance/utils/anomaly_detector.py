# attendance_app/utils/anomaly_detector.py

import pandas as pd
import os
import joblib
import requests

from dotenv import load_dotenv
import os

load_dotenv()



# ãƒ¢ãƒ‡ãƒ«ä¿å­˜å ´æ‰€
MODEL_DIR = 'user_models'

def generate_text_with_api(prompt):
    api_key = os.getenv("API_KEY")
    url = f"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro:generateContent?key={api_key}"
    headers = {
        "Content-Type": "application/json",
    }
    data = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        result = response.json()
        return result["candidates"][0]["content"]["parts"][0]["text"]
    else:
        return None

# æ¨è«–ï¼‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
def predict_and_generate_message(user_id, attendance_row):
    # --- ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆå…±é€šãƒ¢ãƒ‡ãƒ«ï¼‰ ---
    model_path = os.path.join(MODEL_DIR, "motivation_anomaly_model.pkl")
    
    if not os.path.exists(model_path):
        return "ãƒ¢ãƒ‡ãƒ«æœªå­¦ç¿’ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿è“„ç©ä¸­ğŸ“š"

    # --- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ---
    model = joblib.load(model_path)

    # --- ç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼ˆ7å€‹æƒãˆã‚‹ï¼‰ ---
    X = pd.DataFrame([{
        'start_hour': attendance_row.get('start_hour', 0),
        'user_mean_working_minutes': attendance_row.get('user_mean_working_minutes', 480),
        'user_std_working_minutes': attendance_row.get('user_std_working_minutes', 30),
        'before_noon_flag': attendance_row.get('before_noon_flag', 1),
        'yesterday_overtime_flag': attendance_row.get('yesterday_overtime_flag', 0),
        'global_mean_working_minutes': attendance_row.get('global_mean_working_minutes', 470),
        'global_std_working_minutes': attendance_row.get('global_std_working_minutes', 40),
    }])

    X = X.fillna(0)  # å¿µã®ãŸã‚NaNåŸ‹ã‚

    # --- æ¨è«– ---
    prediction = model.predict(X)[0]  # 0: æ­£å¸¸, 1: æ¥ãªã•ã™ã, 2: é ‘å¼µã‚Šã™ã

    # --- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ ---
    if prediction == 1:
        # æ¥ãªã•ã™ã
        prompt = "æœ€è¿‘å‡ºå‹¤ãŒå°‘ãªã‚ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€æ³¨æ„å‹§å‘Šã‚’ã™ã‚‹æ—¥æœ¬èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãä½œæˆã—ã¦ãã ã•ã„ã€‚"
        message = generate_text_with_api(prompt)
        if message:
            return message
        else:
            return "æœ€è¿‘ãŠç–²ã‚Œæ°—å‘³ã‹ãªï¼Ÿç„¡ç†ã›ãšã‚†ã£ãã‚Šãƒšãƒ¼ã‚¹ã‚’å–ã‚Šæˆ»ãã†ã­ï¼"
        
    elif prediction == 2:
        # é ‘å¼µã‚Šã™ã
        prompt = "é ‘å¼µã‚Šã™ãã¦ã„ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€ã­ãã‚‰ã„ã¨ä¼‘æ¯ã‚’ä¿ƒã™æ—¥æœ¬èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãä½œæˆã—ã¦ãã ã•ã„ã€‚"
        message = generate_text_with_api(prompt)
        if message:
            return message
        else:
            return "ã™ã”ãé ‘å¼µã£ã¦ã‚‹ã­ï¼ä½“èª¿ã‚‚å¤§äº‹ã«ã—ãªãŒã‚‰ã€ç„¡ç†ã—ãªã„ã§ã­âœ¨"
        
    else:
        # æ­£å¸¸
        return "ä»Šæ—¥ã‚‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ã„ã¤ã‚‚å®‰å®šã—ãŸå‹¤å‹™ã€ç´ æ™´ã‚‰ã—ã„ã§ã™ğŸ˜Š"

